{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Voting Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVfJCzUQchNh"
      },
      "source": [
        "#KNN\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "dataset = pd.read_csv('data.csv')\n",
        "del dataset[\"id\"]\n",
        "del dataset[\"age_days\"]\n",
        "\n",
        "#redundant data variables\n",
        "display(HTML(dataset.head(10).to_html()))\n",
        "X = dataset.iloc[:, :-1]\n",
        "#print(X)\n",
        "y = dataset.iloc[:, -1]\n",
        "#print(y)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "#print(X_train)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#create new a knn model\n",
        "knn = KNeighborsClassifier()\n",
        "#create a dictionary of all values we want to test for n_neighbors\n",
        "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
        "#use gridsearch to test all values for n_neighbors\n",
        "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
        "#fit model to training data\n",
        "knn_gs.fit(X_train, y_train)\n",
        "\n",
        "#save best model\n",
        "knn_best = knn_gs.best_estimator_\n",
        "#check best n_neigbors value\n",
        "print(knn_gs.best_params_)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#create a new logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "#fit the model to the training data\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#create a new random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "#create a dictionary of all values we want to test for n_estimators\n",
        "params_rf = {'n_estimators': [50, 100, 200]}\n",
        "#use gridsearch to test all values for n_estimators\n",
        "rf_gs = GridSearchCV(rf, params_rf, cv=5)\n",
        "#fit model to training data\n",
        "rf_gs.fit(X_train, y_train)\n",
        "\n",
        "#save best model\n",
        "rf_best = rf_gs.best_estimator_\n",
        "#check best n_estimators value\n",
        "print(rf_gs.best_params_)\n",
        "\n",
        "#test the three models with the test data and print their accuracy scores\n",
        "print('knn: {}'.format(knn_best.score(X_test, y_test)))\n",
        "print('rf: {}'.format(rf_best.score(X_test, y_test)))\n",
        "print('log_reg: {}'.format(log_reg.score(X_test, y_test)))\n",
        "\n",
        "#Building the Voting Classifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "#create a dictionary of our models\n",
        "estimators=[('knn', knn_best), ('rf', rf_best), ('log_reg', log_reg)]\n",
        "#create our voting classifier, inputting our models\n",
        "ensemble = VotingClassifier(estimators, voting='hard')\n",
        "\n",
        "#fit model to training data\n",
        "ensemble.fit(X_train, y_train)\n",
        "#test our model on the test data\n",
        "ensemble.score(X_test, y_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OPeU9v1Kvyf"
      },
      "source": [
        "#predicting the test data\n",
        "y_pred = ensemble.predict(X_test)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqY-DTHOBzqa",
        "outputId": "f5d3b2d4-0873-49bb-ca49-49f092b607e1"
      },
      "source": [
        "#creating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8131 2351]\n",
            " [3371 7099]]\n",
            "Accuracy: 0.7268995799923635\n",
            "Precision: 0.7512169312169312\n",
            "Recall: 0.6780324737344795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9_yK9O_RFX_",
        "outputId": "2e706584-aa75-4f84-fd13-ac3ccd6b0e9b"
      },
      "source": [
        "#per label classification\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.78      0.74     10482\n",
            "           1       0.75      0.68      0.71     10470\n",
            "\n",
            "    accuracy                           0.73     20952\n",
            "   macro avg       0.73      0.73      0.73     20952\n",
            "weighted avg       0.73      0.73      0.73     20952\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}